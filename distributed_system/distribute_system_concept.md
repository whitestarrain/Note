# 1. 基础概念

## 1.1. 互联网架构演进（重要）

### 1.1.1. 单体架构

> 最基本的单体架构，如典型的LAMP；Linux+Apache+Mysql+PHP

![distribute_system-24](./image/distribute_system-24.png)

- 说明：单体架构所有模块和功能都集中在一个项目中 ，部署时也是将项目所有功能部整体署到服务器中

- 优点
  - 小项目上，开发快，成本低
  - 架构简单
  - 易于测试
  - 易于部署
- 缺点
  - 大项目上，模块耦合严重，不易开发、维护，沟通成本高
  - 新增业务困难
  - 核心业务与边缘业务混合在一块，出现问题互相影响

### 1.1.2. 应用服务与数据服务分离

> 随网站业务的发展,越来越多的用户访问,面临的问题
> > 性能越来越差
> >
> > 越来越多的数据导致存储空间不足

![distribute_system-25](./image/distribute_system-25.png)

- 说明：服务器分离，服务器对应不同的硬件需求。不同服务器承担不同角色，性能提升
  - 应用服务器:需要更快更强大的CPU(处理大量的业务逻辑)
  - 数据库服务器:需要更快的硬盘和更大的內存(快速磁盘检索和数据缓存)
  - 文件服务器:需要更大的硬盘(存储大量用户上传的文件)

### 1.1.3. 缓存改善数据库性能

> 迫着用户逐渐增多,网站再次面临挑战，**数据库压力太大导致访问延迟**,进而影响整个网站的性能,用户体验受到影响!

- 本地缓存(ehcache)
  - 优点：快
  - 缺点：数据量小，和应用程序争用内存
- 远程分布式缓存(redis,memcache)
  - 优点：按需扩容，数据量大
  - 缺点：需要网络访问，性能差些

### 1.1.4. 应用服务器集群

> 随着用户逐渐增多,**单一应用服务器**面临新的问题, **能够处理的请求连接有限,网站访问高峰期,应用服务器成为整个网站的瓶颈**。

![distribute_system-26](./image/distribute_system-26.png)

<br />

**[负载均衡](./负载均衡服务器.md)**

### 1.1.5. 数据库读写分离

> 使用缓存后,虽然大大减轻了数据库的读压力,但是面临新的问题， <br />
> **有一部分读操作(缓存访问不命中,缓存过期)和全部的写操作要访问数据库** <br />
> 当用户达到一定规模后,数据库因为负载压力过高而成为整个系统的瓶颈。

![distribute_system-27](./image/distribute_system-27.png)

### 1.1.6. 反向代理和CDN加速

> 用户规模越来越大,发布地堿越来越广,地域网络环境差别很大,面临问题：**如何保证用户的访问体验,不至于因访问慢而流失用户**?

![distribute_system-28](./image/distribute_system-28.png)

- CDN加速服务器：
  - 部署在**当地运行商**
  - 把**静态资源**提前缓存到各地运行商的服务中心中
  - 网络服务会通过当地的运行商进行分发
  - 如果运行商服务中心有对应的资源，就直接返回，不再转发

- 反向代理服务器：
  - 部署在**应用服务器最外层**
  - 负载均衡和反向代理可能是同一个服务器
  - 同样是提前**缓存静态资源**

- 优点：
  - 加快用户访问响应速度
  - 减轻后端服务器压力

### 1.1.7. 分布式文件系统

> 单文件服务器、单数据库服务器,面临问题:**存不下日益增长的数据**

![distribute_system-29](./image/distribute_system-29.png)

- 大数据存储：
  - hdfs(有限制每块最小不能小于1MB)

- 适合存储小文件，图片的分布式文件系统：
  - FaskDFS
  - TFS

- 数据访问模块
  - MyCat
  - Sharding-JDBC

### 1.1.8. 使用NoSQL，搜索引擎

> 随着业务的发展,数据的存储需求和检索需求越来越复杂,面临的问题
> > **存储的字段差异较大,骷髅表(复杂的业务，不是每个字段都有用，到处空字段)**。
> >
> > **复杂的文本检索**

![distribute_system-30](./image/distribute_system-30.png)

- 搜索引擎:解决检索
  - lucene:Apache开源搜索工具包
  - solr:搜索平台，开箱即用
  - **elasticsearch**:同样开箱即用
- NoSQL:解决非结构化数据存储
  - mongodb
  - **elasticsearch**

### 1.1.9. 业务拆分

> 网站越做越好,业务不断扩大,越来越复杂,面临的问题
>
> **应用程序将变得无比庞大,迭代周期越来越快,牵一发而动全身,怎么应对快速的业务发展需要** ?

![distribute_system-31](./image/distribute_system-31.png)

```
如大型电商网站会将首页、商铺、订
单、买家等拆分成不同的产品线,分
归不同的团队负责,分成不同的应用,
独立部署。通过链接、MQ、数据存储
系统建立关联。
```

- 消息中间件用来通信，解耦

### 1.1.10. 分布式服务（服务化）

> 业务规模不断增大,应用拆分越来越小,越来越多,面临的问题 <br />
> 应用间的关系越来越复杂,应用中存在大量相同的业务操作。<br />
> 后端的数据库要被成千上万台应用服务器连接,数据库连接资源不足。<br />

- 说明
  - 将一些连接数据库的业务操作，部署为分布式服务
  - 应用服务访问分布式服务

- 分布式服务（服务化）两种架构方式
  - **SOA**
    > ![distribute_system-32](./image/distribute_system-32.png)
    - 优点：通过消息总线ESB便于管理
    - 缺点：所有服务都连到消息总线ESB，会有总线瓶颈
  - **微服务**
    > ![distribute_system-33](./image/distribute_system-33.png)-
    - 优点：没有中心，服务间可以互调，也可以做负载均衡
    - 缺点：需要进行服务治理

- 服务框架
  - dubbo
  - springCloud
- 配置中心
  - Dubbo:zookeep
  - SpringCloud Config
  - Disconf 百度
  - Config-tookit 当当
  - Diamond 阿里

### 1.1.11. 大数据分析，监控，推荐

> 再往后,需要什么了? <br />
> 数据挖掘、分析、推荐等业务需求,庞大系统的监控、问题分析等需求。

![distribute_system-34](./image/distribute_system-34.png)

- 大数据
  - Hadoop
  - Spark
- 系统监控
  - Zabbix
  - elasticsearch+beats+Kibana

### 1.1.12. 架构设计误区

- 味追随大公司的解决方案
- 为了技术而技术
- 企图用技术解决所有问题
  > 一定要清楚业务，也可以通过业务解决

## 1.2. 分布式基本概念

### 1.2.1. 分布式和集群

简单说，分布式是以缩短单个任务的执行时间来提升效率的，而集群则是通过提高单位时间内执行的任务数来提升效率。

- 区别与联系：
  - 业务分布
    - 分布式是指将不同的业务分布在不同的地方
    - 而集群指的是将几台服务器集中在一起，实现同一业务。
  - 包含关系/联系
    - 分布式中的每一个节点，都可以做集群
    - 而集群并不一定就是分布式的。
  - 容错性
    - 分布式，从窄意上理解，也跟集群差不多， 但是它的组织比较松散，不像集群，有一个组织性，一台服务器垮了，其它的服务器可以顶上来。
    - 分布式的每一个节点，都完成不同的业务，一个节点垮了，那这个业务就不可访问了。

- 示例：秒杀系统
  - 集群

    ![distribute_system-4](./image/distribute_system-4.png)
  - 分布式

    ![distribute_system-5](./image/distribute_system-5.png)

- 加机器就是分布式吗？
  - 加机器更加适用于构建集群，因为它真是只有加机器。
  - 而对于分布式来说，你首先需要将业务进行拆分，然后再加机器（不仅仅是加机器那么简单），同时你还要去解决分布式带来的一系列问题。

### 1.2.2. 微服务

- 概念：
  - 微服务 (Microservices) 是一种软件架构风格
- 核心：
  - 传统的一站式应用，根据业务拆分成一个一个的服务
  - 每一个微服务提供单个业务功能的服务，一个服务做一件事
  - 各功能区块使用与**语言无关** (Language-Independent/Language agnostic) 的 API 集相互通信。
  - 再利用模块化的方式组合出复杂的大型应用程序，
- 优点：
  - 针对特定服务发布，影响小，风险小，成本低
  - 频繁发布版本，快速交付需求
  - 低成本扩容，弹性伸缩，适应云环境
- 缺点/问题
  - 分布式系统的复杂性
    > 包括服务间的依赖，服务如何拆封，内部接口规范，数据传递等等问题
  - 部署，测试和监控的成本问题
    > 分布式系统，部署，测试和监控都需要大量的中间件来支撑，而且中间件本身也要维护，原先单体应用很简单的事务问题 ，转到分布式环境就变得很复杂
  - 分布式事务和CAP的相关问题

- 技术栈

  <details>
  <summary style="color:red;">展开</summary>

  | 微服务条目                             | 落地技术                                                        |
  | -------------------------------------- | --------------------------------------------------------------- |
  | 服务开发                               | Springboot、Spring、SpringMVC                                   |
  | 服务配置与管理                         | Netflix 公司的 Archaius、阿里的 Diamond 等                      |
  | 服务注册与发现                         | Eureka、Consul、Zookeeper 等                                    |
  | 服务调用                               | Rest、RPC、gRPC                                                 |
  | 服务熔断器                             | Hystrix、Envoy 等                                               |
  | 负载均衡                               | Ribbon、Nginx 等                                                |
  | 服务接口调用(客户端调用服务的简化工具) | Feign 等                                                        |
  | 消息队列                               | Kafka、RabbitMQ、ActiveMQ 等                                    |
  | 服务配置中心管理                       | SpringCloudConfig、Chef 等                                      |
  | 服务路由（API 网关）                   | Zuul 等                                                         |
  | 服务监控                               | Zabbix、Nagios、Metrics、Spectator 等                           |
  | 全链路追踪                             | Zipkin，Brave、Dapper 等                                        |
  | 服务部署                               | Docker、OpenStack、Kubernetes 等                                |
  | 数据流操作开发包                       | SpringCloud Stream（封装与 Redis,Rabbit、Kafka 等发送接收消息） |
  | 事件消息总线                           | Spring Cloud Bus                                                |

  </details>

## 1.3. 微服务演进与Service Mesh

**首先概括性说明：Service Mesh 是微服务时代的 TCP/IP 协议。**

> **时代0**

- 开发人员想象中，不同服务间通信的方式，抽象表示如下：

  ![distribute_system-35.png](./image/distribute_system-35.png)

> **时代1：原始通信时代**

![distribute_system-36.png](./image/distribute_system-36.png)

- 现实远比想象的复杂
- 在实际情况中，通信需要底层能够传输字节码和电子信号的物理层来完成
- 在TCP协议出现之前，服务需要自己处理网络通信所面临的丢包、乱序、重试等一系列流控问题
- 因此服务实现中，除了业务逻辑外，还夹杂着对网络传输问题的处理逻辑。

> **时代2：TCP时代**

![distribute_system-37.png](./image/distribute_system-37.png)

- 为了避免每个服务都需要自己实现一套相似的网络传输处理逻辑，TCP协议出现了
- 它解决了网络传输中通用的流量控制问题，将技术栈下移，从服务的实现中抽离出来，成为操作系统网络层的一部分。

> **时代3：第一代微服务**

![distribute_system-38.png](./image/distribute_system-38.png)

- 在TCP出现之后，机器之间的网络通信不再是一个难题，以GFS/BigTable/MapReduce为代表的分布式系统得以蓬勃发展
- 这时，分布式系统特有的通信语义又出现了，如：
  - 熔断策略
  - 负载均衡
  - 服务发现
  - 认证和授权
  - quota限制
  - trace和监控等等
- 于是服务根据业务需求来实现一部分所需的通信语义。

> **时代4：第二代微服务**

![distribute_system-39.png](./image/distribute_system-39.png)

- 为了避免每个服务都需要自己实现一套分布式系统通信的语义功能，随着技术的发展，一些面向微服务架构的开发框架出现了，如
  - Twitter的[Finagle](https://link.zhihu.com/?target=https%3A//finagle.github.io/)、
  - Facebook的[Proxygen](https://link.zhihu.com/?target=https%3A//code.facebook.com/posts/1503205539947302)
  - Spring Cloud
- 这些框架实现了分布式系统通信需要的各种通用语义功能
  - 如负载均衡和服务发现等
  - 因此一定程度上屏蔽了这些通信细节，使得开发人员使用较少的框架代码就能开发出健壮的分布式系统。

> **时代5：第一代Service Mesh**

- 第二代微服务模式(如Spring Cloud)看似完美，但开发人员很快又发现，它也存在一些本质问题：
  - 其一，虽然框架本身屏蔽了分布式系统通信的一些通用功能实现细节，但开发者却要花更多精力去掌握和管理复杂的框架本身，在实际应用中，去追踪和解决框架出现的问题也绝非易事；
  - 其二，开发框架通常只支持一种或几种特定的语言，回过头来看文章最开始对微服务的定义，一个重要的特性就是语言无关，但那些没有框架支持的语言编写的服务，很难融入面向微服务的架构体系，想因地制宜的用多种语言实现架构体系中的不同模块也很难做到；
  - 其三，框架以lib库的形式和服务联编，复杂项目依赖时的库版本兼容问题非常棘手，同时，框架库的升级也无法对服务透明，服务会因为和业务无关的lib库升级而被迫升级；

- 因此出现了第一代Service Mesh，代理模式（边车模式）,如：
  - Linkerd
  - Envoy
  - NginxMesh
- 它将分布式服务的通信抽象为单独一层
  - 实现分布协调功能：在这一层中实现负载均衡、服务发现、认证授权、监控追踪、流量控制等分布式系统所需要的功能
  - 代理服务：
    - 作为一个和服务对等的代理服务，和服务部署在一起，接管服务的流量
    - 通过代理之间的通信间接完成服务之间的通信请求，这样上边所说的三个问题也迎刃而解。

- 示例图
  - 如果我们从一个全局视角来看，就会得到如下部署图

    > ![distribute_system-40.png](./image/distribute_system-40.png)

  - 如果我们暂时略去服务，只看Service Mesh的单机组件组成的网络：

    > ![distribute_system-41.png](./image/distribute_system-41.png)

  - Service Mesh，也就是服务网格了。就像是一个由若干服务代理所组成的错综复杂的网格。

    > ![distribute_system-42.png](./image/distribute_system-42.png)

> **时代6：第二代Service Mesh**

- 演进目的：
  - 为了提供统一的上层运维入口，演化出了集中式的控制面板
  - 所有的单机代理组件通过和控制面板交互进行网络拓扑策略的更新和单机数据的汇报

- 架构图
  - 整体

  > ![distribute_system-43.png](./image/distribute_system-43.png)

  - 只看单机代理组件(数据面板)和控制面板的Service Mesh全局部署视图如下：

    > ![distribute_system-44.png](./image/distribute_system-44.png)

> **总结说明**

- 定义：
  - 服务网格是一个**基础设施层**，用于处理服务间通信。
  - 云原生应用有着复杂的服务拓扑，服务网格保证**请求在这些拓扑中可靠地穿梭**。
  - 在实际应用当中，服务网格通常是由一系列轻量级的**网络代理**组成的
  - 它们与应用程序部署在一起，但**对应用程序透明**。

- 四个关键词：
  - **基础设施层** + **请求在这些拓扑中可靠穿梭**
    - 这两个词加起来描述了Service Mesh的定位和功能
    - 与TCP的定位类似
  - **网络代理**：
    - 这描述了Service Mesh的实现形态；
  - **对应用透明**：
    - 这描述了Service Mesh的关键特点
    - 正是由于这个特点，Service Mesh能够解决以Spring Cloud为代表的第二代微服务框架所面临的三个本质问题；

- 优点总结：
  - 屏蔽分布式系统通信的复杂性(负载均衡、服务发现、认证授权、监控追踪、流量控制等等)，服务只用关注业务逻辑；
  - 真正的语言无关，服务可以用任何语言编写，只需和Service Mesh通信即可；
  - 对应用透明，Service Mesh组件可以单独升级；

- 面临挑战
  - Service Mesh组件以代理模式计算并转发请求，一定程度上会降低通信系统性能，并增加系统资源开销；
  - Service Mesh组件接管了网络流量，因此服务的整体稳定性依赖于Service Mesh，同时额外引入的大量Service Mesh服务实例的运维和管理也是一个挑战；

# 2. 深入前后端分离

待做

[基于NodeJS的全栈式开发](https://www.cnblogs.com/tianyamoon/p/11176640.html)

# 3. 分布式理论

## 3.1. 幂等

幂等性是数学上的含义是对于参数 x，f(x)=f(f(x));比如绝对值函数。 在分布式环境下表示的是对于同样的请求，在一次或者多次请求的情况下对系统的使用资源是一样的。保证失败重试不会导致提交两次。

- 方法：
  - 带版本号的方式；
  - 采用数据库唯一索引方式；

## 3.2. CAP

### 3.2.1. 基本概念

- 一致性
  - 在分布式环境中，一致性是指数据在多个节点之间能够保持一致的特性。
  - 如果在某个节点上执行变更操作后，用户可以立即从其他任意节点上读取到变更后的数据，
  - 那么就认为这样的系统具备强一致性。

- 可用性
  - 可以性是指系统提供的服务必须一直处于可用状态，
  - 对于用户的每一个操作请求总是能够在有限的时间内返回结果。它主要强调以下两点：
    - **有限的时间内**：对于用户的一个请求操作，系统必须要在指定的时间内返回处理结果，如果超过这个时间，那么系统就被认为是不可用的。
    - **返回结果**：不论成功或者失败，都需要明确地返回响应结果。

- 分区容错性
  - 分区容错性指定是分布式系统在遇到网络分区时，仍需要能够对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。
  - **网络分区**：在分布式系统中，由于不同的节点会分布在不同子网中（不同机房或异地网络等），由于一些特殊的原因，可能会出现子网内部是正常的，但子网彼此之间却无法正常通信，从而导致整个系统的网络被切分成若干个独立的区域，这就是网络分区。

### 3.2.2. CAP 理论

- 基本说明：
  - 一个分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容错性（P：Partition tolerance）这三个需求，
  - 最多只能同时满足其中的两个

- 原因：
  - 首先对于一个分布式系统而言，网络分区是不可避免的，不可能永远不出现网络故障，所以分区容错性 P 必须要保证。
  - 假设一个分布式系统中出现了网络分区，如下：

    ![distribute_system-1](./image/distribute_system-1.png)

- 示例说明
  > 假设用户 1 向节点 1 上增加了 10 个数据，但节点 1 和节点 2 之间因为网络分区而无法进行数据同步，碰巧用户 2 此时发起了查询请求，此时有两种处理方案：
  - **放弃 A，保证 C**：即对于用户 2 的查询返回失败，直至节点 1 上的数据同步至节点 2，两者的数据都变为 60 为止；
  - **放弃 C，保证 A**：对于本次的查询直接返回原来的数据 50，此时放弃了一致性，但保证了可用性。待网络恢复后，仍然需要将节点 1 上的数据同步至节点 2。
  - 可以看到无论如何，都是无法既保证 A ，又保证 C 的。

### 3.2.3. 选择策略

- **保证 AC ，放弃 P**：这种情况下可以将所有数据（或者是与事务相关的数据）都放在一个分布式节点上，这样可以避免网络分区带来的影响，但同时也意味着放弃了系统的可扩展性，如单机版本的 MySQL、Oracle 等。
- **保证 CP ，放弃 A**：这种情况下如果发生了网络分区故障，此时节点间的数据就无法同步。因此在故障修复前都需要放弃对外提供服务，直至网络恢复，数据到达一致为止。
- **保证 AP ，放弃 C**：这种情况相当于放弃一致性。具体而言，是放弃数据的强一致性，但保证数据的最终一致性。因为不论是什么系统，数据最终都需要保持一致，否则整个系统就无法使用。在这种策略下，在某个短暂的时间窗口内会存在数据不一致的情况。

![distribute_system-2](./image/distribute_system-2.png)

### 3.2.4. 注意点

- CAP原理是对**分布式数据存储系统**的一个定论
  - 比如我们一个分布式系统各个节点都读写同一个mysql实例，那么对于这个分布式系统来说，讨论CAP原理是没有意义的
  - 因为各个节点之间可以不用因为数据复制而进行通信，满足分区容忍性（P），可以随时响应请求，满足可用性（A），同时因为访问的是一个数据库实例，本身已经保证了数据一致性（C）。
  - 因此，**在讨论CAP原理的时候，更多的是针对那些有数据存储、数据复制场景的分布式存储系统，也就是我们熟悉的NoSql数据库。**
    > 由于我们大多数人都不会去设计一款新的NoSql数据库来使用，更多的是使用现成的NoSql开源系统进行数据的存储，比如Hbase、MongoDB、Cassandra等。所以大多数时候，其实我们都用不上CAP原理。

- **保证了其中2点后，不是就要完全抛弃另外一点。只是相对的要做一些牺牲**
  - 比如在保证CP的情况下，虽然没办法保证高可用性，但这不意味着可用性为0
  - 我们可以通过合理的设计尽量的提高可用性，让可用性尽可能的接近100%。
  - 同理，在AP的情况下，也可以尽量的保证数据的一致性，或者实现弱一致性，即最终一致性。

## 3.3. BASE

### 3.3.1. 基本说明

BASE是对基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventually Consistent）三个短语的简写，它是对 CAP 理论中 AP 策略的延伸。其核心是即便无法做到强一致性，但每个系统应用都应该根据自身业务的特点，采取适当的方式来保证系统的最终一致性，而具体的方案就体现在这三个短语上：

### 3.3.2. 基本可用

基本可用是指分布式系统在出现不可预知的故障时，允许损失部分可用性，例如：

- 延长响应时间：比如原来的的查询只需要 0.5 秒，现在延长到 1～ 2 秒；
- 服务降级：比如在商品秒杀时，部分用户会被引导到一个降级页面。

### 3.3.3. 软状态

软状态也称为弱状态，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统整体的可用性，即允许不同节点间的数据同步存在延时。

### 3.3.4. 最终一致性

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终需要达到一致的状态。

# 4. 通信设计

## 4.1. RPC

### 4.1.1. 说明

- 简单说明：**RPC（Remote Procedure Call）**
  - 即远程过程调用，关注的是**远程调用**而非本地调用。
    > 也就是说两台服务器A，B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，
  - 由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。

- 目的/作用：
  - **RPC 的出现就是为了让你调用远程方法像调用本地方法一样简单。**
  - 屏蔽了底层网络编程的具体细节。

- 使用场景
  - 由于 RPC 固有的消耗相对本地调用高出几个数量级，本地调用的固有消耗是纳秒级，而 RPC 的固有消耗是在毫秒级。
  - 那么对于**过于轻量的计算任务就并不合适导出远程接口由独立的进程提供服务**
  - 只有**花在计算任务上时间远远高于 RPC 的固有消耗才值得导出为远程接口提供服务**。

### 4.1.2. 原理

![distribute_system-7](./image/distribute_system-7.png)
![distribute_system-8](./image/distribute_system-8.png)

- 服务消费方（client）调用以本地调用方式调用服务；
- client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；
- client stub找到服务地址，并将消息发送到服务端；
- server stub收到消息后进行解码；
- server stub根据解码结果调用本地的服务；
- 本地服务执行并将结果返回给server stub；
- server stub将返回结果打包成消息并发送至消费方；
- client stub接收到消息，并进行解码；
- 服务消费方得到最终结果。

### 4.1.3. RPC主要考虑的问题

#### 4.1.3.1. Call ID映射

- 说明
  - 在本地调用中，函数体是直接通过方法指针来指定的
  - 再远程调用中，两个进程的地址空间是不同的，无法使用方法指针
  - 因此需要其他方法来指定远程方法

- 要求
  - 在RPC中，**所有的函数都必须有自己的一个ID**。
  - 这个ID在所有进程中都是**唯一确定**的。
  - 客户端在做远程过程调用时，必须附上这个ID。
  - 然后我们还需要在客户端和服务端分别维护一个 {函数 <–> Call ID} 的对应表。
  - 两者的表不一定需要完全相同，但相同的函数对应的Call ID必须相同。

- 基本流程
  - 当客户端需要进行远程调用时，它就查一下这个表，找出相应的Call ID，然后把它传给服务端
  - 服务端也通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。

  ![distribute_system-9](./image/distribute_system-9.png)

#### 4.1.3.2. 网络传输/协议

- 说明
  - 远程调用往往用在网络上，客户端和服务端是通过网络连接的
  - 所有的数据都需要通过网络传输，因此就需要有一个网络传输层。

- 要求：
  - 网络传输层需要把Call ID和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。
  - 只要能完成这两者的，都可以作为传输层使用。
  - 因此，它所**使用的协议其实是不限**的，能完成传输就行
  - 尽管大部分RPC框架都使用TCP协议，但其实UDP也可以，而gRPC干脆就用了HTTP2。

<details>
<summary style="color:red;">问题：既有 HTTP ,为啥用 RPC 进行服务调用</summary>

- RPC 只是一种概念、一种设计，就是为了解决 不同服务之间的调用问题, 它一般会包含有 **传输协议** 和 **序列化协议** 这两个。
- 但是，HTTP 是一种协议，RPC框架可以使用 **HTTP协议作为传输协议**或者**直接使用TCP作为传输协议**，使用不同的协议一般也是为了适应不同的场景。
</details>

#### 4.1.3.3. 序列化问题/协议

- 说明
  - 在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。
  - 但是在远程过程调用时，客户端跟服务端是不同的进程，不能通过内存来传递参数
    > 甚至有时候客户端和服务端使用的都不是同一种语言（比如服务端用C++，客户端用Java或者Python）。
  - 因此需要通过网络传输的方式把数据发送过去。数据以什么格式传输，就涉及到了序列化，反序列化

- 要求
  - 客户端把参数先转成一个字节流（编码），传给服务端
  - 服务端再把字节流转成自己能读取的格式（解码）
  - 这个过程叫**序列化和反序列化**
  - 同理，从服务端返回的值也需要序列化反序列化的过程。

- 为什么需要序列化？
  - 1.转换为字节流方便进行网络传输。
  - 2.实现跨平台、跨语言；

- 常见序列化方式
  - XML（Extensible Markup Language）是一种常用的序列化和反序列化协议，具有跨机器，跨语言等优点。狭义web service就是基于SOAP消息传递协议（一个基于XML的可扩展消息信封格式）来进行数据交换的。
  - Hessian是一个动态类型，简洁的，可以移植到各个语言的二进制序列化对象协议。采用简单的结构化标记、采用定长的字节记录值、采用引用取代重复遇到的对象。
  - JSON（Javascript Object Notation）起源于弱类型语言Javascript， 是采用"Attribute－value"的方式来描述对象协议。与XML相比，其协议比较简单，解析速度比较快。
  - Protocol Buffers 是google提供的一个开源序列化框架，是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。同 XML 相比， Protobuf 的主要优点在于性能高。它以高效的二进制方式存储，比 XML 小 3 到 10 倍，快 20 到 100 倍。
  - Thrift 既是rpc框架，同时也具有自己内部定义的传输协议规范(TProtocol)和传输数据标准(TTransports)，通过IDL脚本对传输数据的数据结构(struct) 和传输数据的业务逻辑(service)根据不同的运行环境快速的构建相应的代码，并且通过自己内部的序列化机制对传输的数据进行简化和压缩提高高并发、 大型系统中数据交互的成本。

### 4.1.4. 常见RPC框架

- **RMI（JDK自带）：** JDK自带的RPC，有很多局限性，不推荐使用。

  <details>
  <summary style="color:red;">说明</summary>

  ![distribute_system-10](./image/distribute_system-10.png)
  ![distribute_system-11](./image/distribute_system-11.png)

  - stub(桩)：
    - stub实际上就是远程过程在客户端上面的一个代理proxy。
    - 当我们的客户端代码调用API接口提供的方法的时候，RMI生成的stub代码块会将请求数据序列化，交给远程服务端处理，
    - 然后将结果反序列化之后返回给客户端的代码。
    - 这些处理过程，对于客户端来说，基本是透明无感知的。
  - remote：
    - 这层就是底层网络处理了，RMI对用户来说，屏蔽了这层细节
    - stub通过remote来和远程服务端进行通信。
  - skeleton(骨架)：
    - 和stub相似，skeleton则是服务端生成的一个代理proxy。
    - 当客户端通过stub发送请求到服务端，则交给skeleton来处理，其会根据指定的服务方法来反序列化请求，然后调用具体方法执行
    - 最后将结果返回给客户端。
  - registry(服务发现)：
    - 借助JNDI发布并调用了rmi服务。
    - 实际上，JNDI就是一个注册表，服务端将服务对象放入到注册表中，客户端从注册表中获取服务对象。
    - rmi服务，在服务端实现之后需要注册到rmi server上，然后客户端从指定的rmi地址上lookup服务，调用该服务对应的方法即可完成远程方法调用。
    - registry是个很重要的功能，当服务端开发完服务之后，要对外暴露，如果没有服务注册，则客户端是无从调用的，即使服务端的服务就在那里。

  </details>

- **Dubbo:** Dubbo是 阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成。目前 Dubbo 已经成为 Spring Cloud Alibaba 中的官方组件。
- **gRPC** ：gRPC是可以在任何环境中运行的现代开源高性能RPC框架。它可以通过可插拔的支持来有效地连接数据中心内和跨数据中心的服务，以实现负载平衡，跟踪，运行状况检查和身份验证。它也适用于分布式计算的最后一英里，以将设备，移动应用程序和浏览器连接到后端服务。
- **Hessian：** Hessian是一个轻量级的remotingonhttp工具，使用简单的方法提供了RMI的功能。 相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据。
- **Thrift：** Apache Thrift是Facebook开源的跨语言的RPC通信框架，目前已经捐献给Apache基金会管理，由于其跨语言特性和出色的性能，在很多互联网公司得到应用，有能力的公司甚至会基于thrift研发一套分布式服务框架，增加诸如服务注册、服务发现等功能。

### 4.1.5. 异常处理

- 本地调用和 RPC 调用的一些差异：
  - 本地调用一定会执行，而远程调用则不一定，调用消息可能因为网络原因并未发送到服务方。
  - 本地调用只会抛出接口声明的异常，而远程调用还会跑出 RPC 框架运行时的其他异常。
  - 本地调用和远程调用的性能可能差距很大，这取决于 RPC 固有消耗所占的比重。

- 异常分析：
> 正是上面那些区别决定了使用 RPC 时需要更多考量
 - 当调用远程接口抛出异常时，异常可能是一个**业务异常**， 也可能是 RPC 框架抛出的**运行时异常**（如：网络中断等）。
 - 业务异常：表明服务方已经执行了调用，可能因为某些原因导致未能正常执行，
 - RPC 运行时异常：则有可能服务方根本没有执行

对调用方而言的异常处理策略自然需要区分。

### 4.1.6. rpc框架设计思路

- 最最基本的设计思路
  - 上来你的服务就得去注册中心注册吧，你是不是得有个注册中心，保留各个服务的信息，可以用 zookeeper 来做，对吧。
  - 然后你的消费者需要去注册中心拿对应的服务信息吧，对吧，而且每个服务可能会存在于多台机器上。
  - 接着你就该发起一次请求了，咋发起？当然是基于动态代理了，你面向接口获取到一个动态代理，这个动态代理就是接口在本地的一个代理，然后这个代理会找到服务对应的机器地址。
  - 然后找哪个机器发送请求？那肯定得有个负载均衡算法了，比如最简单的可以随机轮询是不是。
  - 接着找到一台机器，就可以跟它发送请求了，第一个问题咋发送？你可以说用 netty 了，nio 方式；第二个问题发送啥格式数据？你可以说用 hessian 序列化协议了，或者是别的，对吧。然后请求过去了。
  - 服务器那边一样的，需要针对你自己的服务生成一个动态代理，监听某个网络端口了，然后代理你本地的服务代码。接收到请求的时候，就调用对应的服务代码，对吧。

- 三个问题：
  - Call ID映射可以直接使用函数字符串，也可以使用整数ID。映射表一般就是一个哈希表。
  - 序列化反序列化可以自己写，也可以使用Protobuf或者FlatBuffers之类的。
  - 网络传输库可以自己写socket，或者用asio，ZeroMQ，Netty之类。

## 4.2. RESTful

待做

<!--file:///D:/learn/githubRepo/JavaGuide/docs/system-design/coding-way/RESTfulAPI%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B.md-->

## 4.3. RPC和REST对比

# 5. 分布式算法

## 5.1. 分布式事务算法(共识算法)

### 5.1.1. 2PC(两阶段提交)

### 5.1.2. 3PC(三阶段提交)

### 5.1.3. paxos

#### 5.1.3.1. 说明

`Paxos` 算法是基于**消息传递且具有高度容错特性的一致性算法**，是目前公认的解决分布式一致性问题最有效的算法之一，**其解决的问题就是在分布式系统中如何就某个值（决议）达成一致** 。

- Paxos中的角色：
  - `Proposer提案者`
  - `Acceptor表决者`
  - `Learner学习者`

- Paxos的两个阶段：
  - `Prepare` 阶段
  - `accept` 阶段

#### 5.1.3.2. Prepare阶段

> 也就是广播编号，没什么流程

- `Proposer提案者`：
  - 负责提出 `proposal提案`
  - 每个提案者在提出提案时都会首先获取到一个 **具有全局唯一性的、递增的提案编号N**，即在整个集群中是唯一的编号 N，
  - 然后将该编号赋予其要提出的提案，在**Prepare阶段是只将提案编号发送给所有的表决者**。

  ```
  简而言之：
    就是提出一个编号N。（诶呀，想要内容之后再说，编号N符合条件才行啊）
    这个编号作为之后要做提案(对应value)的编号
    然后发给其他表决者
  ```
- `Acceptor表决者`：
  - 每个表决者仅会 `accept(批准)` 编号大于自己本地 `maxN` 的提案，
    - 如果批准了，返回`(节点id,null,null)`
    - 如果不批准，返回`(节点id,maxN,value)`
  - 每个表决者在 `accept` 某提案后，会将该提案编号N记录在本地，
  - 这样每个表决者中保存的已经被 accept 的提案中会存在一个**编号最大的提案**，其编号假设为 `maxN`。

  ```
  简而言之：
    就是仅会接收更大的提案编号，并且保存下来
    如果批准提案(接收N)，就返回(节点id,null,null)（兄弟，你给的N够大，我要了，我这里也没什么让你看的了）
    如果不批准提案(不接收N)，就返回(节点id,maxN,value)（兄弟，你也太弱了，我可不要你，让你看看我这里最大的提案编号）
  ```

![zookeeper-15](./image/zookeeper-15.png)

#### 5.1.3.3. accept阶段

- Prepare阶段，我(`Proposer提案者`)给那么多`Acceptor表决者`发了这个提案，他们觉得咋样啊？
  - 如果 `Proposer` 收到了超过半数的 `Acceptor` 的批准（`Proposer` 本身同意）， 那么此时 `Proposer` 会给所有的 `Acceptor` 发送真正的提案
  - 可以理解为第一阶段为试探
  - 第一阶段通过后，`Proposer` 就会发送提案的内容和提案编号。

- 我(`Proposer提案者`)这次可给你们发了真正的提案，这是最后一次确定了呦
  - 表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，
  - 如果该提案编号 **大于等于** 已经批准过的最大提案编号
    - 那么就 `accept` 该提案（**此时执行提案内容但不提交**）
    - 随后将情况返回给 `Proposer` 。
  - 如果不满足则不回应或者返回 NO 。

  ![zookeeper-16](./image/zookeeper-16.png)

- 我（`Proposer提案者`）收到回复了，让我看看要怎么搞啊。
  - **情况1**：当 `Proposer` 收到超过半数的 `accept`
    - 那么它这个时候会向所有的 `acceptor` 发送提案的提交请求。
    - 需要注意的是：
      - 因为上述仅仅是超过半数的 `acceptor` 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，
      - 所以这个时候需要**向未批准的 acceptor 发送提案内容和提案编号并让它无条件执行和提交**，
      - 而对于前面已经批准过该提案的 `acceptor` 来说 **仅仅需要发送该提案的编号** ，让 `acceptor` 执行提交就行了。

    ![zookeeper-17](./image/zookeeper-17.png)

  - **情况2**：如果 `Proposer` 如果没有收到超过半数的 `accept`
    - 那么它将会将 **递增** 该 `Proposal` 的编号，然后 **重新进入 Prepare 阶段** 。

#### 5.1.3.4. Leader的学习

对于 `Learner` 来说如何去学习 `Acceptor` 批准的提案内容，这有很多方式，读者可以自己去了解一下，这里不做过多解释。

#### 5.1.3.5. 死循环问题

- 说明：
  - 比如说，此时提案者 P1 提出一个方案 M1，完成了 `Prepare` 阶段的工作，这个时候 `acceptor` 则批准了 M1
  - 但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 `Prepare` 阶段的工作。
  - 然后 P1 的方案已经不能在第二阶段被批准了（因为 `acceptor` 已经批准了比 M1 更大的 M2），
  - 所以 P1 自增方案变为 M3 重新进入 `Prepare` 阶段，然后 `acceptor` ，又批准了新的 M3 方案，
  - 它又不能批准 M2 了，这个时候 M2 又自增进入 `Prepare` 阶段
  - 一直循环往复。。。

  就这样无休无止的永远提案下去，这就是 `paxos` 算法的死循环问题。

- 解决方案: 就允许一个能提案

### 5.1.4. ZAB

- ZAB 协议全称就是 ZooKeeper Atomic Broadcast protocol，是 ZooKeeper 用来实现一致性的算法
- ZAB 基于 Paxos 算法实现的，不过 ZAB 对 Paxos 进行了很多改进与优化。
- 两者的设计目标也存在差异
  - ZAB 协议主要用于构建一个高可用的分布式数据主备系统
  - 而 Paxos 算法则是用于构建一个分布式的一致性状态机系统。

- 详细算法：[跳转](../Middleware/zookeeper.md)

### 5.1.5. Raft

<!-- TODO: 把看完的资料整理一下吧 -->

#### 5.1.5.1. leader election

> **初次选举**

![git_test](./image/distribute_system-45.gif)

> **再次选举**

#### 5.1.5.2. log replication

#### 5.1.5.3. safety

#### 5.1.5.4. corner case

### 5.1.6. 应用

#### 5.1.6.1. mysql的两段式提交

> **简单说明**

![distribute_system-12](./image/distribute_system-12.png)

- mysql两段式提交
  - 就是我们先把这次更新写入到redolog中，并设redolog为prepare状态
  - 然后再写入binlog,写完binlog之后再提交事务，并设redolog为commit状态
  - 也就是把relolog拆成了prepare和commit两段
  - **2PC保证了事务在引擎层（redo）和server层（binlog）之间的原子性**

- 具体过程
  - prepare阶段:redo持久化到磁盘（redo group commit），并将回滚段置为prepared状态，此时binlog不做操作。

    ![distribute_system-12](./image/distribute_system-12.png)-
  - commit阶段:innodb释放锁，释放回滚段，设置提交状态，binlog持久化到磁盘，然后存储引擎层提交
    > 其中binlog作为XA协调器，即以binlog是否成功写入磁盘作为事务提交的标志（innodb commit标志并不是事务成功与否的标志）

    ![distribute_system-13](./image/distribute_system-13.png)

- 目的：
  - 其实redolog是后来才加上的，binlog是之前就有的。
  - 一开始存储引擎只有MyISAM,后来才有的InnoDB,然后MyISAM没有事务，没有crash-safe的能力
  - 所以InnoDB搞了个redolog。**然后为了保证两份日志同步，所以才有了两段式提交**。

> **详细**

[Mysql笔记，XA章节](../database/mysql.md)

#### 5.1.6.2. zookeeper 的ZAB使用paxos

[跳转](../Middleware/zookeeper.md)

#### 5.1.6.3. Mysql:5.7开始，支持group replication，采用Paxos

#### 5.1.6.4. MongoDB:从3.4开始，支持类raft复制协议

## 5.2. 分布式缓存算法

### 5.2.1. 传统hash算法局限性

> **情景**

- 说明：使用传统hash算法将三个单词放到三个节点中
- 测试代码
  <details>
  <summary style="color:red;">展开</summary>

  ```java
  public class SimpleHash {
      private int cap;
      private int seed;

      public SimpleHash(int cap, int seed) {
          this.cap = cap;
          this.seed = seed;
      }

      public int hash(String value) {
          int result = 0;
          int len = value.length();
          for (int i = 0; i < len; i++) {
              result = seed * result + value.charAt(i);
          }
          return (cap - 1) & result;
      }

      public static void main(String[] args) {
          SimpleHash simpleHash = new SimpleHash(2 << 12, 8);
          System.out.println("node_number=hash(\"semlinker\") % 3 -> " +
            simpleHash.hash("semlinker") % 3);
          System.out.println("node_number=hash(\"kakuqo\") % 3 -> " +
            simpleHash.hash("kakuqo") % 3);
          System.out.println("node_number=hash(\"test\") % 3 -> " +
            simpleHash.hash("test") % 3);
      }
  }
  ```
  </details>
- 结果图示
  > ![distribute_system-14](./image/distribute_system-14.png)

> **业务场景**

- 缓存节点减少：
  - 说明：在分布式多节点系统中，出现故障很常见，任何节点都可能在没有任何事先通知的情况下挂掉，针对这种情况我们期望系统只是出现性能降低，正常的功能不会受到影响。
  - 传统hash算法表现：假设其中 1 个节点出现故障，这时节点数发生了变化，节点个数从 3 减少为 2，此时表格的状态发生了变化：
    > ![distribute_system-15](./image/distribute_system-15.png)
  - 弊端：
    - 很明显节点的减少会**导致键与节点的映射关系发生变化**
      > 以 “semlinker” 为例，变化前系统有 3 个节点，该键对应的节点编号为 1，
      > 当出现故障时，节点数减少为 2 个，此时该键对应的节点编号为 0。
    - 这个变化对于新的键来说并不会产生任何影响，**但对于已有的键来说，将导致节点映射错误**
    - **会导致大量的请求无法命中缓存(也就是大量缓存失效,缓存雪崩)，使DB过载**

- 缓存节点增加
  - 同理

### 5.2.2. 一致性hash算法

> **说明**

- 一致性哈希算法在 1997 年由麻省理工学院提出，是一种特殊的**哈希算法**
- 特征：**在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系**
- 解决问题：一致性哈希解决了简单哈希算法在分布式哈希表（Distributed Hash Table，DHT）中存在的动态伸缩等问题 。

> **优点**

- 可扩展性：
  - 一致性哈希算法**保证了增加或减少服务器时，数据存储的改变最少**
  - 相比传统哈希算法大大节省了数据移动的开销。
- 更好地适应数据的快速增长：
  - 情景：采用一致性哈希算法分布数据，当数据不断增长时，部分虚拟节点中可能包含很多数据、造成数据在虚拟节点上分布不均衡
  - 解决：
    - 此时可以将包含数据多的虚拟节点分裂，这种分裂仅仅是将原有的虚拟节点一分为二、不需要对全部的数据进行重新哈希和划分。
    - 虚拟节点分裂后，如果物理服务器的负载仍然不均衡，只需在服务器之间调整部分虚拟节点的存储分布。
    - 这样可以随数据的增长而动态的扩展物理服务器的数量，且代价远比传统哈希算法重新分布所有数据要小很多。

> **一致性哈希算法与哈希算法的关系**

- 一致性哈希算法是在哈希算法基础上提出的，在动态变化的分布式环境中，哈希算法应该满足的几个条件
  - 平衡性：是指 hash 的结果应该平均分配到各个节点，这样从算法上解决了负载均衡问题。
  - 单调性：是指在新增或者删减节点时，不影响系统正常运行。
  - 分散性：是指数据应该分散地存放在分布式集群中的各个节点（节点自己可以有备份），不必每个节点都存储所有的数据。

> **算法说明**

- 数据结构：
  - 一致性哈希算法通过一个叫作一致性哈希环的数据结构实现。
  - 这个环的起点是 0，终点是 2^32 - 1，并且起点与终点连接，故这个环的整数分布范围是 [0, 2^32-1]
    > ![distribute_system-16](./image/distribute_system-16.png)

- 对象放入到哈希环
  - 将四个对象放到hash环
  ```
  hash(o1) = k1; hash(o2) = k2;
  hash(o3) = k3; hash(o4) = k4;
  ```
  > ![distribute_system-17](./image/distribute_system-17.png)

- 为对象选择服务器
  - 接着使用同样的哈希函数，我们将服务器也放置到哈希环上
  - 可以选择服务器的 IP 或主机名作为键进行哈希，这样每台服务器就能确定其在哈希环上的位置。
  - 这里假设我们有 3 台缓存服务器，分别为 cs1、cs2 和 cs3：
  ```
  hash(cs1) = t1; hash(cs2) = t2; hash(cs3) = t3; # Cache Server
  ```
  > ![distribute_system-18](./image/distribute_system-18.png)

- 服务器增加情况
  - 假设由于业务需要，我们需要增加一台服务器 cs4，经过同样的 hash 运算，该服务器最终落于 t1 和 t2 服务器之间
  - 对于上述的情况，只有 t1 和 t2 服务器之间的对象需要重新分配。在以上示例中只有 o3 对象需要重新分配，即它被重新到 cs4 服务器。
  - 在前面我们已经分析过，如果使用简单的取模方法，当新添加服务器时可能会导致大部分缓存失效，而使用一致性哈希算法后，这种情况得到了较大的改善，因为只有少部分对象需要重新分配。
  > ![distribute_system-19](./image/distribute_system-19.png)
- 服务器减少情况
  - 假设 cs3 服务器出现故障导致服务下线，这时原本存储于 cs3 服务器的对象 o4，需要被重新分配至 cs2 服务器，其它对象仍存储在原有的机器上。
  > ![distribute_system-20](./image/distribute_system-20.png)

- 虚拟节点
  - 出现原因:
    -但对于新增服务器的情况还存在一些问题。新增的服务器 cs4 只分担了 cs1 服务器的负载，
    服务器 cs2 和 cs3 并没有因为 cs4 服务器的加入而减少负载压力。
    如果 cs4 服务器的性能与原有服务器的性能一致甚至可能更高，那么这种结果并不是我们所期望的。
  - 解决
    - 针对这个问题，我们可以通过引入虚拟节点来解决负载不均衡的问题。
    - 即将每台物理服务器虚拟为一组虚拟服务器，将虚拟服务器放置到哈希环上，
    - 如果要确定对象的服务器，需先确定对象的虚拟服务器，再由虚拟服务器确定物理服务器。

    > ![distribute_system-21](./image/distribute_system-21.png)
    >
    > 图中 o1 和 o2 表示对象，v1 ~ v6 表示虚拟服务器，s1 ~ s3 表示物理服务器。

> **代码实现**

<details>
<summary style="color:red;">实现(无虚拟节点)</summary>

```java
import java.util.SortedMap;
import java.util.TreeMap;

public class ConsistentHashingWithoutVirtualNode {
    //待添加入Hash环的服务器列表
    private static String[] servers = {"192.168.0.1:8888", "192.168.0.2:8888",
      "192.168.0.3:8888"};

    //key表示服务器的hash值，value表示服务器
    private static SortedMap<Integer, String> sortedMap = new TreeMap<Integer, String>();

    //程序初始化，将所有的服务器放入sortedMap中
    static {
        for (int i = 0; i < servers.length; i++) {
            int hash = getHash(servers[i]);
            System.out.println("[" + servers[i] + "]加入集合中, 其Hash值为" + hash);
            sortedMap.put(hash, servers[i]);
        }
    }

    //得到应当路由到的结点
    private static String getServer(String key) {
        //得到该key的hash值
        int hash = getHash(key);
        //得到大于该Hash值的所有Map
        SortedMap<Integer, String> subMap = sortedMap.tailMap(hash);
        if (subMap.isEmpty()) {
            //如果没有比该key的hash值大的，则从第一个node开始
            Integer i = sortedMap.firstKey();
            //返回对应的服务器
            return sortedMap.get(i);
        } else {
            //第一个Key就是顺时针过去离node最近的那个结点
            Integer i = subMap.firstKey();
            //返回对应的服务器
            return subMap.get(i);
        }
    }

    //使用FNV1_32_HASH算法计算服务器的Hash值
    private static int getHash(String str) {
        final int p = 16777619;
        int hash = (int) 2166136261L;
        for (int i = 0; i < str.length(); i++)
            hash = (hash ^ str.charAt(i)) * p;
        hash += hash << 13;
        hash ^= hash >> 7;
        hash += hash << 3;
        hash ^= hash >> 17;
        hash += hash << 5;

        // 如果算出来的值为负数则取其绝对值
        if (hash < 0)
            hash = Math.abs(hash);
        return hash;
    }

    public static void main(String[] args) {
        String[] keys = {"semlinker", "kakuqo", "fer"};
        for (int i = 0; i < keys.length; i++)
            System.out.println("[" + keys[i] + "]的hash值为" + getHash(keys[i])
                    + ", 被路由到结点[" + getServer(keys[i]) + "]");
    }
}
```
</details>

### 5.2.3. hash slot分块算法

[Redis 使用的虚拟槽分区只是一致性哈希算法的变种](https://www.reddit.com/r/redis/comments/4yztxi/whichoneisbetterhashslotor_consistent/)

> Both Redis / Cassandra still use consistent hashing. They just consistent hash to virtual nodes / hashslots. Having this extra level of indirection allows for migrating these virtual abstractions, while still keeping the hashing consistent.
>
> This allows for things like dynamically scaling the cluster. Normally if you consistently hash between say 4 hosts, you'll need to do a lot of work to rescale the data to a different number of hosts as the existing hash to one of 4 slots would all need to be recalculated.
>
> Instead you can start with a large number of virtual hashslots all living on the same host, and migrate a portion of them to another host as you add boxes, but without ever needing to recalculate the number of slots you are hashing too as the number of hosts change.

**[redis笔记](../database/redis.md)**

### 5.2.4. 应用

## 5.3. 分布式复制模型

TODO: 分布式复制模型 理论整理，之后再融到整个文件中。

### 5.3.1. 复制的目的

- 在分布式系统中，数据通常需要被分散在多台机器上，主要为了达到以下目的：
  - **扩展性**
    - 数据量因读写负载巨大，一台机器无法承载
    - 数据分散在多台机器上可以有效地进行负载均衡，达到灵活的横向扩展。
  - **容错、高可用**
    - 在分布式系统中，单机故障是常态，在单机故障下仍然希望系统能够正常工作，这时候就需要数据在多台机器上做冗余
    - 在遇到单机故障时其他机器就可以及时接管。
  - **统一的用户体验**
    - 如果系统客户端分布在多个地域，通常考虑在多个地域部署服务
    - 以方便用户能够就近访问到他们所需要的数据，获得统一的用户体验。

- 数据的多机分布的方式主要有两种
  - 一种是将数据 **分片保存** ，每个机器保存数据的部分分片（Kafka中称为Partition，其他部分系统称为Shard）
  - 另一种则是 **完全的冗余** ，其中每一份数据叫做一个副本（Kafka中称为Replica），通过数据复制技术实现
  - 在分布式系统中，两种方式通常会 **共同使用**
    - 最后的数据分布往往是下图的样子，一台机器上会保存不同数据分片的若干个副本

  ![distribute_system-45](./image/distribute_system-45.png)

- 复制的目标需要保证 **若干个副本上的数据是一致的**
  - 这里的“一致”是一个十分不确定的词
  - 既可以是不同副本上的数据在任何时刻都保持完全一致
  - 也可以是不同客户端不同时刻访问到的数据保持一致
  - 一致性的强弱也会不同，表现也不一样
    - 有可能需要任何时候不同客端都能访问到相同的新的数据
    - 也有可能是不同客户端某一时刻访问的数据不相同，但在一段时间后可以访问到相同的数据。(最终一致性)
  - 一致性的强弱主要有两个考量点，第一是性能，第二则是复杂性。
    - 性能比较好理解
      - 因为冗余的目的不完全是为了高可用，还有延迟和负载均衡这类提升性能的目的
      - 如果只一味地为了地强调数据一致，可能得不偿失。
    - 复杂性是因为分布式系统中，有着比单机系统更加复杂的不确定性
      - 节点之间由于采用不大可靠的网络进行传输，并且不能共享统一的一套系统时间和内存地址（后文会详细进行说明）
      - 这使得原本在一些单机系统上很简单的事情，在转到分布式系统上以后就变得异常复杂
      - 这种复杂性和不确定性甚至会让我们怀疑，这些副本上的数据真的能达成一致吗

### 5.3.2. 基本模型

#### 5.3.2.1. 主从复制

- 对复制而言，最直观的方法就是将副本赋予不同的角色
  - 其中有一个主副本，主副本将数据存储在本地后，将数据更改作为日志，或者以更改流的方式发到各个从副本（后文也会称节点）中
  - 在这种模式下，所有写请求就全部会写入到主节点上，读请求既可以由主副本承担也可以由从副本承担
  - 这样对于读请求而言就具备了扩展性，并进行了负载均衡
- 但这里面存在一个 **权衡点** ，就是 **客户端视角看到的一致性问题**
  - 这个权衡点存在的核心在于，数据传输是通过网络传递的， **数据在网络中传输的时间是不能忽略的** 。

![distribute_system-46](./image/distribute_system-46.png)

> 如上图所示，在这个时间窗口中，任何情况都有可能发生。
> 在这种情况下，客户端何时算写入完成，会决定其他客户端读到数据的可能性。

- 这里我们假设这份数据有 **一个主副本和一个从副本**
  - 如果主副本保存后即向客户端返回成功，这样叫做 **异步复制** （1）。
  - 而如果等到数据传送到从副本1，并得到确认之后再返回客户端成功，称为 **同步复制** （2）。
  - 这里我们先假设系统正常运行，在异步同步下
    - 如果从副本承担读请求，假设reader1和reader2同时在客户端收到写入成功后发出读请求
    - 两个reader就可能读到不一样的值。
  - 为了避免这种情况，实际上有两种角度的做法
    - 第一种角度是让客户端 **只从主副本读取数据**
      - 这样，在正常情况下，所有客户端读到的数据一定是一致的（Kafka当前的做法）
      - 但如果仅采用异步复制并由主副本承担读请求，当主节点故障发生切换时，一样会发生数据不一致的问题。
    - 另一种角度则是 **采用同步复制**
      - 假设使用纯的同步复制，当有多个副本时，任何一个副本所在的节点发生故障，都会使写请求阻塞
      - 同时每次写请求都需要等待所有节点确认，如果副本过多会极大影响吞吐量
- 很多系统会 **把这个决策权交给用户** ，这里以Kafka为例
  - 首先提供了同步与异步复制的语义（通过客户端的acks参数确定），另外提供了ISR机制，而只需要ISR中的副本确认即可
  - 系统可以容忍部分节点因为各种故障而脱离ISR，那样客户端将不用等待其确认，增加了系统的容错性
  - 当前Kafka未提供让从节点承担读请求的设计，但在高版本中已经有了这个Feature
  - 这种方式使系统有了更大的灵活性，用户可以根据场景自由权衡一致性和可用性。

#### 5.3.2.2. 多节点复制

#### 5.3.2.3. 无主节点复制

### 5.3.3. 分布式系统挑战

#### 5.3.3.1. 部分失效

#### 5.3.3.2. 分布式系统特有的故障

#### 5.3.3.3. 不可靠的时钟

### 5.3.4. 分布式算法的理论模型

## 5.4. 分布式id算法

TODO: 分布式id算法

[6 个流行的分布式 ID 方案之间的对决](https://catcat.cc/post/2020-09-19/)

### 5.4.1. 基本说明

- 基本说明：
  - 分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。
  - 我们如何为不同的数据节点生成全局唯一主键呢？这个时候，我们就需要为我们的系统引入分布式 id 了。

- 可选方案：
  - **UUID**：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。
  - **数据库自增 id** : 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。
  - **利用 redis 生成 id :** 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。
  - **Twitter的snowflake算法** ：Github 地址：https://github.com/twitter-archive/snowflake。
  - **美团的Leaf分布式ID生成系统** ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。感觉还不错。美团技术团队的一篇文章：https://tech.meituan.com/2017/04/21/mt-leaf.html 。

### 5.4.2. snowflake (Twitter)

### 5.4.3. leaf (美团)

## 5.5. 其他重要分布式协议

### 5.5.1. Gossip 协议

<p style="color:red;">
待做
</p>

# 6. 分布式锁

## 6.1. Msyql实现

## 6.2. Redis实现

[实现](../database/redis.md)

## 6.3. Zookeeper实现

[实现](../Middleware/zookeeper.md)

## 6.4. 锁的比较

- Redis分布式锁
  - 说明
    - 它获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能。
  - 缺点
    - 另外来说的话，Redis 的设计定位决定了它的数据并不是强一致性的，在某些极端情况下，可能会出现问题。锁的模型不够健壮。（比如哨兵模式下，Master宕机，可能出现两个客户端获得锁）
    - Redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。
  - 使用情况
    - 但是另一方面使用 Redis 实现分布式锁在很多企业中非常常见，而且大部分情况下都不会遇到所谓的“极端复杂场景”。
    - 所以使用 Redis 作为分布式锁也不失为一种好的方案，最重要的一点是 Redis 的性能很高，可以支撑高并发的获取、释放锁操作。

- ZK 分布式锁:
  - 说明
    - ZK 天生设计定位就是分布式协调，强一致性。锁的模型健壮、简单易用、适合做分布式锁。
    - 如果获取不到锁，只需要添加一个监听器就可以了，不用一直轮询，性能消耗较小。
  - 缺点
    - 如果有较多的客户端频繁的申请加锁、释放锁，对于 ZK 集群的压力会比较大。

- 一些建议
  - 如果公司里面有 ZK 集群条件，优先选用 ZK 实现
  - 但是如果说公司里面只有 Redis 集群，没有条件搭建 ZK 集群。 那么其实用 Redis 来实现也可以
  - 另外还可能是系统设计者考虑到了系统已经有 Redis，但是又不希望再次引入一些外部依赖的情况下，可以选用 Redis。这个是要系统设计者基于架构来考虑了。

# 7. 高可用

## 7.1. 集群

## 7.2. 负载均衡算法

- 1、轮询法
  - 将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器
  - 不关心服务器实际的连接数和当前的系统负载。
- 2、随机法
  - 通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。
  - 由概率统计理论可以得知，随着客户端调用服务端的次数增多，
  - 其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。
- 3、源地址哈希法
  - 源地址哈希的思想是根据获取客户端的IP地址，通过哈希函数计算得到的一个数值，
  - 用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。
  - 采用源地址哈希法进行负载均衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。
- 4、加权轮询法
  - 不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。
  - 加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。
    - 给配置高、负载低的机器配置更高的权重，让其处理更多的请；
    - 而配置低、负载高的机器，给其分配较低的权重，降低其系统负载
- 5、加权随机法
  - 与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重
  - 不同的是，它是按照权重随机请求后端服务器，而非顺序。
- 6、最小连接数法
  - 最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，
  - 它是根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求
  - 尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。

## 7.3. 限流算法

待做

<!--file:///D:/learn/githubRepo/JavaGuide/docs/system-design/high-availability/limit-request.md-->

### 7.3.1. 固定窗口计数器算法

### 7.3.2. 滑动窗口计数器算法

### 7.3.3. 漏桶算法

### 7.3.4. 令牌桶算法

## 7.4. 超时和重试机制

## 7.5. 熔断机制

## 7.6. 异步调用

## 7.7. 使用缓存

# 分布式数据结构

TODO: CRDTs (无冲突复制数据类型)

## CRDTs (无冲突复制数据类型)

CRDTs 的应用场景:

- 分布式系统：在多个节点间同步数据时，CRDTs 提供了一种有效的方法来处理网络延迟和分区。
- 协作编辑：CRDTs非常适合于实时协作编辑工具，如在线文档编辑器，它们可以确保不同用户的更改不会相互冲突。
- 离线应用：在需要支持离线工作的应用中，CRDTs 允许用户在没有网络连接时进行操作，随后在重新连接时同步更改。

### CvRDTs（Convergent Replicated Data Types）

通过同步整个状态来保证最终一致性。适用于带宽不是问题，但延迟容忍度较高的场景。

### CmRDTs（Commutative Replicated Data Types）

通过传播操作（而非状态）来实现一致性。适合于网络带宽有限，但需要更快速应答的场景。

# 8. 常见问题

# 9. 参考资料

- [ ] [Java全栈知识体系](https://pdai.tech/md/algorithm/alg-domain-load-balance.html)
- [ ] [图解一致性哈希算法](https://segmentfault.com/a/1190000021199728)
- [ ] [万字带你入门Zookeeper](https://juejin.cn/post/6844904045283377165)
- [ ] [springcloud：RPC和HTTP ](https://www.cnblogs.com/flypig666/p/11699526.html)
- [ ] [【RPC】SpringCloud简介 & RPC与Restful API关系（三）](https://blog.csdn.net/weixin_33724659/article/details/92518863)
- [x] [The Secret Lives of Data(raft算法演示)](http://thesecretlivesofdata.com/raft/)
- [ ] [一文搞懂Raft算法](https://www.cnblogs.com/xybaby/p/10124083.html)
- [x] [「图解Raft」让一致性算法变得更简单](https://zinglix.xyz/2020/06/25/raft/)
- [ ] [微服务架构设计](https://gudaoxuri.gitbook.io/microservices-architecture/)
- [ ] [什么是 Service Mesh](https://zhuanlan.zhihu.com/p/61901608)
- [ ] [缓存一致性最佳实践](https://mp.weixin.qq.com/s/vuqFtP1NWR2wi4VgtoLT9A)
- [ ] [分布式系统设计之共识算法—2PC、3PC、 Paxos ](https://blog.51cto.com/u_15654567/5327089)
- [ ] [Replication（上）：常见复制模型&分布式系统挑战](https://tech.meituan.com/2022/08/25/replication-in-meituan-01.html)
- [ ] [Replication（下）：事务，一致性与共识](https://tech.meituan.com/2022/08/25/replication-in-meituan-02.html)
- [ ] [《数据密集型应用系统设计（DDIA）》]()
- [ ] [阿里微服务生态体系](https://seata.io/zh-cn/)
- [ ] [分布式数据库的一致性问题与共识算法](https://thiscute.world/posts/consistency-and-consensus-algorithm/)


